## 线性回归

假设函数：$h_{\theta}(x) = \theta_0x_0+ \theta_1 x_1 + \dots + \theta_n x_n$

损失函数：$J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2$

目标函数：$\min J(\theta_0,\theta_1,\dots,\theta_n)$



线性回归中可能遇到的问题：

- 求解损失函数的最小值的两种方法：梯度下降法和正规方程。
- 特征缩放：对特征数据进行归一化操作，一是能够提升模型的收敛速度，二是能提升模型精度。
- 学习率$\alpha$的选取：过小，导致迭代次数变多，收敛速度变慢；过大，可能会跳过最优解，最终导致无法收敛。



过拟合问题：

 [1.jfif](/回归问题/1.jfif) 

解决方法：1）丢弃一些对最终预测影响不大的特征，可以通过PCA算法实现。

​                    2）使用正则化，保留所有特征，但是减少特征前面的参数$\theta$的大小，具体即修改线性回归中的损失函数形式。岭回归和Lasso回归即如此。



## 岭回归和Lasso回归

是为了解决线性回归出现的过拟合问题以及在通过正规方程求解$\theta$过程中出现的X转置乘以X转置不可逆这两类问题。

这两种回归均通过在损失函数中引入正则化项来达到目的。

线性回归损失函数：$J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2$

岭回归损失函数：$J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2 + \lambda\sum\limits_{j=1}^n \theta_j^2$

Lasso回归损失函数：$J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2 + \lambda \sum\limits_{j=1}^n|\theta_j|$

其中$\lambda$是正则化参数，过大，会使得所有参数$\theta$均最小化，造成欠拟合；过小，会导致对过拟合处理不当。



两者主要区别：

岭回归引入的是L2范数惩罚项，Lasso回归引入的是L1范数惩罚项，Lasso回归能够使得损失函数中许多的$\theta$均变为0，岭回归要求所有的$\theta$均存在，	Lasso回归的计算量要远远小于岭回归。

![2](/回归问题/2.png)

![3](/回归问题/3.png)

数学描述：

{% asset_img 6.png 数学描述 %}

惩罚方法：

![7](/回归问题/7.png)

Lasso方法：

![8](/回归问题/8.png)

Ridge方法：

![9](/回归问题/9.png)

图像比较：

![4](/回归问题/4.png)

以二维数据为例，左图对应Lasso方法，右图对应Ridge方法，红色部分代表随着$\lambda$的变化所得到的残差平方和。$\hat{\beta}$为椭圆的中心点，对应普通线性模型的最小二乘估计。

两图的区别在于蓝色区域（约束域），等高线和约束域的切点就是目标函数的最优解，Ridge方法对应的约束域是圆，切点只会存在于圆周上，不会和坐标轴相切，在任一维度上的取值都不会为0，因此没有稀疏。

Lasso方法，约束域是正方形，会存在与坐标轴的切点，使得部分维度特征权重为0，容易产生稀疏的结果。

对比可知，Lasso方法可以达到变量选择的效果，



## 维数灾难

高维数据：数据维度远大于样本量的个数。

**维数灾难的特征**

在空间中数据是非常稀疏的，与空间的维数相比样本量总是显得非常少

随着维数的增长，分析所需要的空间样本数会呈指数增长

在高维数据空间，预测将变得不再容易

导致模型过拟合。

![5](/回归问题/5.png)

curse of dimensionality，“维数灾难”，分类器的性能随着特征的个数的变化不断的增加，过了某个值后，性能不升反降。

特征数量越多，训练样本会越稀疏，分类器的参数估计越不准确，更加容易出现过拟合问题；“维数灾难”的另一个影响是训练样本的稀疏性并不是均匀分布的，处于中心位置的训练样本比四周的训练样本更加稀疏。

在高维特征空间内，大多数的训练样本位于超立方体的角落里。

在高维特征空间内，对于样本距离的度量失去意义，由于分类器的性能基本依赖于如Euclidean距离、Manhattan距离等，所以在特征数量过大时，分类器性能降低。

**维数灾难的解决办法**

理论上，如果训练样本数足够大，不会出现维数灾难，可以采用任意多的特征来训练分类器。

对于一些泛化能力不是很好的分类器，如Neural network ,knn,decision tree等，不应该采用过多的特征，这些需要精确的非线性决策边界。

对于一些泛化能力较好的分类器，如naive bayes,linear classifier等，可以适当增加特征的数量。

给定N个特征，从中选出M个最优的特征，有很多特征选择算法来帮助确定特征数量以及选择特征，有很多特征提取算法，如PCA等，交叉验证（cross-validation）也长被用于检测和避免过拟合问题。

[机器学习中的维数灾难问题](https://www.linuxidc.com/Linux/2016-01/127857.htm)



**数据降维**

维数灾难带来的过拟合问题，解决思路：1）增加样本量；2）减少样本特征。

主成分分析是保留所有原变量的基础上，通过原变量的线性组合得到主成分，选取少数主成分就可保留原变量的绝大部分信息，这样用几个主成分来代替原变量，从而达到降维的目的。

主成分只适用于数据空间维度小于样本量的情况，当数据空间维度很高时，不再使用。





## 逻辑回归

logistic regression（LR）主要用于解决分类问题，逻辑回归是一个非线性模型，sigmoid函数，又称为逻辑回归函数，但是本质上是一个线性回归模型，因为除去sigmoid映射函数关系，其他的步骤，算法都是线性回归。

逻辑回归首先把样本映射到[0,1]之间的数值（归功于sigmoid，可以把任何连续的值映射到[0,1]之间，数越大越趋向于0，越小越趋近于1。）

sigmoid函数为：$g(z)= \frac{1}{1+e^{-z}}$

判定边界：对多元线性回归方程求sigmoid函数。$h_{\theta}(x)= g(\theta_0+\theta_1x_1+ \dots + \theta_n x_n)$找到一组θ，把样本分为两类，得到判定边界。



**逻辑回归的损失函数**
$$
Cost(h_{\theta}(x),y) = \cases{-\log(h_{\theta}(x)), y=1 \\ -\log(1-h_{\theta}(x)), y=0    }
$$
其中$h(x)$是一个概率值，$y=1$表示正样本，$y=0$表示负样本。当为正样本时，如果给定的概率特别小（预测为负样本），损失会很大；给定的概率很大（预测为正样本），损失会接近0。



**正则化**

使用正则化来避免过拟合，在经验风险最小化的基础上（训练误差最小化），尽可能采用简单的模型，可以有效提高泛化预测精度，如果模型过于复杂，变量值稍微有点变动，就会引起预测精度问题。

正则化降低了特征的权重，使得模型更为简单，一般采用L1范式或者L2范式，形式分别为$\Phi(\omega) = ||x||_1$和$\Phi(x) = ||x||_2$。

带L2正则项的损失函数为：
$$
J(\theta) = \left[-\frac{1}{m} \sum\limits_{i=1}^{m} y^{(i)} \log (h_{\theta}(x^{(i)})) + (1-y^{(i)}) \log (1-h_{\theta}(x^{(i)}))    \right] +\frac{\lambda}{m} \sum\limits_{j=1}^n \theta_j^2
$$

给loss function加上正则项，能够使得优化目标函数$h = f + ||\omega||$，需要在两者之间做一个平衡，通过降低模型的复杂度，得到更小的泛化误差，降低过拟合程度。

L1正则化是在loss function后边加正则项为L1范数，容易得到稀疏解（0比较多）。L2正则项所加的为L2范数的平方，加上L2正则项相比于L1，得到解比较平滑（不是稀疏），但是同样能够保证解接近于0（但不等于0，所以相对平滑）的维度比较多，降低模型的复杂度。



[逻辑回归](https://zhuanlan.zhihu.com/p/74874291)





**样本处理**

如果样本太大，离散化后用one-hot编码处理成0，1值，再用LR处理会较快收敛，如果一定用连续值，可以scaling。

如果样本不均衡，样本充足的情况下可以做 下采样——抽样。样本不足的情况下做 上采样——对样本少的及做重复。修改损失函数，给不同的权重。

**特征处理**

离散化优点：映射到高维空间，用linear的LR（快，且兼具更好的分割性）

稀疏性：0，1向量内积乘法运算速度快，计算结果方便存储，容易扩展

离散化后，给线性模型带来一定的非线性，模型稳定性，收敛性高，鲁棒性好，在一定程度上降低了过拟合风险。







